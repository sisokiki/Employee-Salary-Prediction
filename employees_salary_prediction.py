# -*- coding: utf-8 -*-
"""Employees Salary Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b5M0hEa9aKj-tRp7LTfeOmD8nmfo_Jw_
"""

# 1. Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import joblib # For saving and loading models

# 2. Load the dataset
data = pd.read_csv("/content/adult 3.csv")

# 3. Display the first few rows of the dataset to understand its structure
print("3. First 5 rows of the dataset:")
print(data.head())
print("\n" + "="*50 + "\n")

# 4. Get a concise summary of the DataFrame, including data types and non-null values
print("4. Dataset Information:")
data.info()
print("\n" + "="*50 + "\n")

# 5. Check for missing values (represented as '?' in this dataset)
# Replace '?' with NaN for proper missing value handling
print("5. Replacing '?' with NaN and checking for missing values:")
data.replace('?', np.nan, inplace=True)
print(data.isnull().sum())
print("\n" + "="*50 + "\n")

# 6. Handle missing values by filling with the mode for categorical columns
# Identifying categorical columns (object type)
categorical_cols = data.select_dtypes(include=['object']).columns

print("6. Filling missing categorical values with the mode:")
for col in categorical_cols:
    if data[col].isnull().any(): # Check if there are any NaN values in the column
        mode_val = data[col].mode()[0]
        data[col].fillna(mode_val, inplace=True)
        print(f"  Filled missing values in '{col}' with mode: '{mode_val}'")

print("\nMissing values after imputation:")
print(data.isnull().sum())
print("\n" + "="*50 + "\n")

# 7. Preprocessing the target variable 'income'
# Convert income to numerical: <=50K to 0, >50K to 1
print("7. Encoding the 'income' target variable:")
data['income'] = data['income'].apply(lambda x: 1 if x.strip() == '>50K' else 0)
print("Unique values in 'income' after encoding:", data['income'].unique())
print("\n" + "="*50 + "\n")

# 8. Separate features (X) and target (y)
X = data.drop('income', axis=1)
y = data['income']
print("8. Features (X) and Target (y) separated.")
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)
print("\n" + "="*50 + "\n")

# 9. Identify numerical and categorical features for further preprocessing
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns
print("9. Identified Numerical Features:", list(numerical_features))
print("Identified Categorical Features:", list(categorical_features))
print("\n" + "="*50 + "\n")

# Store original categorical feature names before one-hot encoding for later use
original_categorical_features = list(categorical_features)

# 10. Apply One-Hot Encoding to categorical features
# This converts categorical variables into a format that can be provided to ML algorithms
print("10. Applying One-Hot Encoding to categorical features...")
X = pd.get_dummies(X, columns=categorical_features, drop_first=True)
print("Shape of X after One-Hot Encoding:", X.shape)
print("First 5 rows of X after encoding:")
print(X.head())
print("\n" + "="*50 + "\n")

# 11. Apply Feature Scaling to numerical features using StandardScaler
# Scaling helps algorithms that are sensitive to the magnitude of features
print("11. Applying Feature Scaling to numerical features...")
scaler = StandardScaler()
X[numerical_features] = scaler.fit_transform(X[numerical_features])
print("First 5 rows of X after scaling numerical features:")
print(X.head())
print("\n" + "="*50 + "\n")

# 12. Split the data into training and testing sets
# 80% for training, 20% for testing. stratify=y ensures equal proportion of income classes in train/test sets.
print("12. Splitting data into training and testing sets (80/20 split)...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")
print("\n" + "="*50 + "\n")

# 13. Initialize the RandomForestClassifier model
# RandomForest is an ensemble method that generally performs well and is robust to overfitting.
print("13. Initializing RandomForestClassifier model...")
model = RandomForestClassifier(random_state=42)

# 14. Train the model on the training data
print("14. Training the model...")
model.fit(X_train, y_train)
print("Model training complete.")
print("\n" + "="*50 + "\n")

# 15. Make predictions on the test data
print("15. Making predictions on the test data...")
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1] # Probabilities for the positive class (income >50K)
print("Predictions made.")
print("\n" + "="*50 + "\n")

# 16. Evaluate the model's performance
print("16. Evaluating the model's performance:")
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# Visualize the Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['<=50K', '>50K'], yticklabels=['<=50K', '>50K'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()
print("\n" + "="*50 + "\n")

# 17. Hyperparameter Tuning (Optional, but recommended for higher accuracy)
# This step can significantly improve model performance by finding the best parameters.
# Note: GridSearchCV can be computationally intensive and take a long time on large datasets.
# For a quicker run, you might reduce the param_grid or use RandomizedSearchCV.

print("17. Starting Hyperparameter Tuning with GridSearchCV (This may take a while)...")
param_grid = {
    'n_estimators': [100, 200], # Number of trees in the forest
    'max_depth': [10, 20, None], # Maximum depth of the tree
    'min_samples_split': [2, 5], # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2] # Minimum number of samples required to be at a leaf node
}

grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                           param_grid=param_grid,
                           cv=3, # Number of folds for cross-validation
                           n_jobs=-1, # Use all available cores
                           verbose=2, # Verbosity level
                           scoring='accuracy') # Metric to optimize

grid_search.fit(X_train, y_train)

print("\nBest parameters found by GridSearchCV:")
print(grid_search.best_params_)

print("\nBest score (accuracy) on training set:")
print(f"{grid_search.best_score_:.4f}")

# 18. Evaluate the model with the best parameters on the test set
print("\n18. Evaluating the model with best parameters on the test set:")
best_model = grid_search.best_estimator_
y_pred_tuned = best_model.predict(X_test)
y_pred_proba_tuned = best_model.predict_proba(X_test)[:, 1] # Probabilities for the positive class

accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
print(f"Accuracy with tuned model: {accuracy_tuned:.4f}")

print("\nClassification Report with tuned model:")
print(classification_report(y_test, y_pred_tuned))

print("\nConfusion Matrix with tuned model:")
cm_tuned = confusion_matrix(y_test, y_pred_tuned)
print(cm_tuned)

# Visualize the Confusion Matrix for the tuned model
plt.figure(figsize=(8, 6))
sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Greens', cbar=False,
            xticklabels=['<=50K', '>50K'], yticklabels=['<=50K', '>50K'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Tuned Model)')
plt.show()
print("\n" + "="*50 + "\n")

# 19. Feature Importance Analysis
# Understanding which features are most influential in the model's predictions
print("19. Analyzing Feature Importance:")
feature_importances = pd.Series(best_model.feature_importances_, index=X_train.columns)
feature_importances = feature_importances.sort_values(ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x=feature_importances.values, y=feature_importances.index, palette='viridis')
plt.title('Feature Importances (Tuned RandomForest Model)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()
print("Top 10 most important features:")
print(feature_importances.head(10))
print("\n" + "="*50 + "\n")

# 20. ROC Curve and AUC Score
# Visualizing the trade-off between true positive rate and false positive rate
print("20. Plotting ROC Curve and calculating AUC Score:")
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_tuned)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()
print(f"Area Under the Curve (AUC): {roc_auc:.4f}")
print("\n" + "="*50 + "\n")

# 21. Saving the Trained Model
# Saving the best model and the scaler for future use
print("21. Saving the trained model and scaler...")
model_filename = 'best_salary_predictor_model.joblib'
scaler_filename = 'scaler.joblib'

joblib.dump(best_model, model_filename)
joblib.dump(scaler, scaler_filename)

print(f"Model saved as '{model_filename}'")
print(f"Scaler saved as '{scaler_filename}'")
print("\n" + "="*50 + "\n")

# 22. Loading the Model and Scaler (Demonstration)
# How to load the saved model and scaler
print("22. Demonstrating loading the saved model and scaler...")
loaded_model = joblib.load(model_filename)
loaded_scaler = joblib.load(scaler_filename)
print("Model and scaler loaded successfully.")
print("\n" + "="*50 + "\n")

# 23. Example Prediction on New Data
# Create a hypothetical new data point and predict its income
print("23. Performing an example prediction on new hypothetical data:")

# Create a dictionary for a hypothetical new employee
# Ensure all original features are present and in the correct order/type
new_employee_data = {
    'age': 35,
    'workclass': 'Private',
    'fnlwgt': 200000,
    'education': 'Bachelors',
    'educational-num': 13,
    'marital-status': 'Married-civ-spouse',
    'occupation': 'Exec-managerial',
    'relationship': 'Husband',
    'race': 'White',
    'gender': 'Male',
    'capital-gain': 5000,
    'capital-loss': 0,
    'hours-per-week': 45,
    'native-country': 'United-States'
}

# Convert to DataFrame
new_data_df = pd.DataFrame([new_employee_data])

print("Hypothetical new employee data:")
print(new_data_df)

# Preprocess the new data using the same steps as the training data
# 1. One-Hot Encode categorical features
# We need to ensure new_data_df has the same columns as X_train after one-hot encoding.
# Use reindex to align columns, filling missing with 0 (for categories not present in new data)
new_data_df_encoded = pd.get_dummies(new_data_df, columns=original_categorical_features, drop_first=True)

# Align columns with the training data's columns (X_train.columns)
# This is crucial because new data might not have all categories present in training data
# or might have categories not seen in training data.
missing_cols = set(X_train.columns) - set(new_data_df_encoded.columns)
for c in missing_cols:
    new_data_df_encoded[c] = 0
# Ensure the order of columns is the same as X_train
new_data_df_encoded = new_data_df_encoded[X_train.columns]

# 2. Scale numerical features
new_data_df_encoded[numerical_features] = loaded_scaler.transform(new_data_df_encoded[numerical_features])

# Make prediction
predicted_income_class = loaded_model.predict(new_data_df_encoded)
predicted_income_proba = loaded_model.predict_proba(new_data_df_encoded)[:, 1]

income_status = ">50K" if predicted_income_class[0] == 1 else "<=50K"

print(f"\nPredicted income for the new employee: {income_status}")
print(f"Probability of earning >50K: {predicted_income_proba[0]:.4f}")
print("\n" + "="*50 + "\n")

accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
print(f"Accuracy with tuned model: {accuracy_tuned:.4f}")